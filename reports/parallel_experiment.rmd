---
title: "Parallelisation Experiment"
date: "`r Sys.Date()`"
params:
  HwInfo: (Hardware Info)
  OsInfo: (OS Info)
  Machine: (Machine Info)
  GoInfo: (GO Info)
  GitHash: (GithubKey)
  StateDB: (StateDB)
  VM: (VM)
  db: ./test.db
---

```{r, include = FALSE}
library(ggplot2)
library(dplyr)
library(RSQLite)
con <- dbConnect(SQLite(), params$db)
data <- dbReadTable(con, 'parallelprofile')
reducedData <- dbReadTable(con, 'aggregatedParallelProfile')
parallelisable <- data %>% filter(speedup > 1.0)
dbDisconnect(con)
```

## 1. Experiment
The experiment is run on the machine `r params$Machine`, which is a `r params$HwInfo` computer.
The operating system is `r params$OsInfo`.
The system has installed `r params$GoInfo`.
The github hash of the Aida repository is `r params$GitHash`.
For this experiment, we use **`r params$StateDB`** as a StateDB and **`r params$VM`** as a virtual machine.
The data set for this experiment is stored in the database `r params$db`.

The experiment was conducted for the block range from **`r format(min(data$block),big.mark=",")`**  to **`r format(max(data$block),big.mark=",")`**.

## 2. Degree of Parallelisation

The parallelisation experiment simulates the parallel execution of transactions inside a block.
We assume the best possible/theoretical scheduling called clairvoyance scheduling.
The schedule is constructed with the [substate work](https://usenix.org/conference/atc21/presentation/kim-yeonsoo).
Two transactions inside a block interfere if they use the same contract addresses.
Note that this information is available after the execution of a smart contract only.
A concrete implementation requires a prediction/speculative execution mechanism requiring extra overheads, drastically diminishing the presented speedup values.
In this experiment, the overheads of a prediction/speculative execution mechanism are not considered.
Hence, the presented numbers are best-case scenarios and are not realisable in a concrete implementation.

We have `r format(nrow(data), big.mark=",")` blocks, of which `r format(nrow(parallelisable), big.mark=",")` are parallelisable.
The number of strictly sequential blocks (not parallelisable) is `r format(100.0*(nrow(data)-nrow(parallelisable))/nrow(data), digits=4)`%.
The maximal achievable parallelism in a block is `r format(max(data$speedup), digits=4)`.
The geometric mean of the speedup is **`r format(exp(mean(log(data$speedup))), digits=4)`**.

We show the distribution of the speedup for blocks that are not sequential.
Sequential blocks whose speedup is one are omitted from the following distribution:

```{r, echo = FALSE, message=FALSE}
hist(parallelisable$speedup, main="Speedup Distribution without Sequential Blocks", breaks = quantile(parallelisable$speedup, 0:10 / 10), xlab="Speedup", col="lightblue1")
lines(density(parallelisable$speedup), col="dodgerblue3", lwd=2)
```

The following figure shows the speedup over block height.
For this distribution, we did not filter the sequential blocks, i.e., all blocks are included for the following timeline:

```{r, echo = FALSE, message=FALSE}
reducedData %>%
  ggplot(aes(x = block, y = speedup)) +
  geom_smooth(color = "tomato") +
  geom_point(alpha=0.3) +
  labs(x="Block Height", y="Speedup")
```

## 3. Number of Transactions per Block

We have, on average `r mean(data$numTx)` transactions per block.
The smallest number of transactions per block is `r min(data$numTx)`, and the largest number is `r max(data$numTx)`.
The distribution of number of transactions per block is shown below:

```{r, echo = FALSE, message=FALSE}
hist(data$numTx, freq=FALSE, main="Number of Transaction Distribution", xlab="Number of Transactions", col="lightblue1"); rug(data$numTx)
abline(v=mean(data$numTx), col="dodgerblue3", lty=2, lwd=2)
```

The smoothened trend line for the number of transaction is shown below. In this diagram, we aggregated the number
of transactions per block for 100K blocks shown as points.

```{r, echo=FALSE, message=FALSE}
reducedData %>%
  ggplot(aes(x = block, y = numTx)) +
  geom_smooth(color = "tomato") +
  geom_point(alpha=0.3) +
  labs(x="Block Height", y="Number of Transactions", title="Number of Transactions per Block")
```

## 4. Processor Utilisation / Efficiency

We measure the processor utilisation of a block by constructing the task graph for a block.
The width of the task graph is a proxy for the maximum number of processors that may be required to achieve the speedup.
The ratio between speedup and width gives an upper bound on the processor utilisation (aka. efficiency).
However, a precise measure of efficiency is NP-hard problem and cannot be computed in polynomial time.

The average number of processors is `r mean(data$ubNumProc)` to execute the task graphs optimally with a maximum of `r max(data$ubNumProc)`.
The lower bound of the processor utilisation is `r format(100.0*exp(mean(log(data$speedup/data$ubNumProc))), digits=4)`% on average, with a maximum of `r max(data$ubNumProc)` processors (upper bound).  A processor utilisation histogram for only parallelisable blocks (i.e. blocks whose speed-up is greater than one) is shown below,

```{r, echo = FALSE, message=FALSE}
parallelisable$utilisation <- parallelisable$speedup / parallelisable$ubNumProc
hist(parallelisable$utilisation, main="Processor Utilisation/Effienciy for Best Speedup", breaks = quantile(parallelisable$utilisation, 0:10 / 10), xlab="Processor Utilisation", col="lightblue1")
lines(density(parallelisable$utilisation), col="dodgerblue3", lwd=2)
```

## 5. Linear Prediction Model

```{r, include = FALSE}
lmSequentialTx <- lm(tSequential~numTx, data=data)
lmCriticalTx <- lm(tCritical~numTx, data=data)
coeffCriticalTx <- lmCriticalTx$coefficients
coeffSequentialTx <- lmSequentialTx$coefficients
a1 <- coeffSequentialTx["numTx"]
b1 <- coeffCriticalTx["numTx"]
a0 <- coeffSequentialTx["(Intercept)"]
b0 <- coeffCriticalTx["(Intercept)"]
tCommit <- mean(data$tCommit)
speedup <-  exp(mean(log(data$speedup)))
tn <- function (s) {
    (a0 - s * b0 + (1-s) * tCommit)/ (s * b1 - a1)
}
X <- seq(speedup, a1/b1*0.95, 0.01)
Y <- tn(X)
```

The linear speedup prediction model estimates a maximum speedup of `r format(a1/b1, digits=4)` assuming an infinite number
of processors. The underlying assumption of the prediction model is that the degree of parallisation (ratio of sequential
transaction execution time and critical path) does not change with an increasing number of transactions.

The number of transactions required for a given speedup is shown in the figure below,

```{r, echo=FALSE, message=FALSE}
plot(X,Y, type="l", lwd=2, col="tomato", main="Linear Prediction Model", xlab="Speedup", ylab="# Tx")
```

Note that the number of transactions grows infinitely reaching the maximum speedup of `r format(a1/b1, digits=4)`.

## 6. Sequential Block Time

The average block execution time is `r mean(data$tBlock)/1e6` milliseconds with a minimum of `r min(data$tBlock)/1e3` microseconds and a maximum of `r max(data$tBlock)/1e9` seconds.
The smoothened trend line for block time (in milliseconds) is shown below. In the diagram, we aggregated the block time for all 100K blocks shown as points.

```{r, echo=FALSE, message=FALSE}
reducedData %>%
  ggplot(aes(x = block, y = tBlock)) +
  geom_smooth(color = "tomato") +
  geom_point(alpha=0.3) +
  labs(x="Block Height", y="Block Time (ms)", title="Sequential Block Time")
```

## 7. Commit Time

The average commit time is `r mean(data$tCommit)/1e6` milliseconds with a minimum of `r min(data$tCommit)/1e3` microseconds and a maximum of `r max(data$tCommit)/1e9` seconds.
The smoothened trend line of the commit time in milliseconds is shown below. In the diagram, we aggregated the commit time for all 100K blocks shown as points.

```{r, echo = FALSE, message=FALSE}
reducedData %>%
  ggplot(aes(x = block, y = tCommit)) +
  geom_smooth(color = "tomato") +
  geom_point(alpha=0.3) +
  labs(x="Block Height", y="Commit Time (ms)", title="Commit Time")
```

## 7. Block gas

The average gas consumption of a block is `r mean(data$gasBlock)` with a minimum of `r min(data$gasBlock)` and a maximum of `r max(data$gasBlock)`.
The smoothened trend line of the gas consumption is shown in the figure below. In the figure, we aggregate the gas consumption for all 100K blocks as points.

```{r, echo = FALSE, message=FALSE}
reducedData %>%
  ggplot(aes(x = block, y = gasBlock)) +
  geom_smooth(color = "tomato") +
  geom_point(alpha=0.3) +
  labs(x="Block Height", y="Block Gas", title="Gas Consumption of Blocks")
```
