---
title: "Parallelisation Experiment"
date: "`r Sys.Date()`"
params:
  HwInfo: (Hardware Info)
  OsInfo: (OS Info)
  Machine: (Machine Info)
  GoInfo: (GO Info)
  GitHash: (GithubKey)
  StateDB: (StateDB)
  VM: (VM)
  db: ./test.db
---

```{r, include = FALSE}
library(ggplot2)
library(dplyr)
library(RSQLite)
con <- dbConnect(SQLite(), params$db)
data <- dbReadTable(con, 'parallelprofile')
reducedData <- dbReadTable(con, 'aggregatedParallelProfile')
parallelisable <- data %>% filter(speedup > 1.0)
dbDisconnect(con)
```

## 1. Experiment
The experiment is run on the machine `r params$Machine`, which is a `r params$HwInfo` computer.
The operating system is `r params$OsInfo`.
The system has installed `r params$GoInfo`.
The github hash of the Aida repository is `r params$GitHash`.
For this experiment, we use **`r params$StateDB`** as a StateDB and **`r params$VM`** as a virtual machine.
The data set for this experiment is stored in the database `r params$db`.

The experiment was conducted for the block range from **`r format(min(data$block),big.mark=",")`**  to **`r format(max(data$block),big.mark=",")`**.

## 2. Degree of Parallelisation

The parallelisation experiment simulates the parallel execution of transactions inside a block.
We assume the best possible/theoretical scheduling called clairvoyance scheduling.
The schedule is constructed with the [substate work](https://usenix.org/conference/atc21/presentation/kim-yeonsoo).
Two transactions inside a block interfere if they use the same contract addresses.
Note that this information is available after the execution of a smart contract only.
A concrete implementation requires a prediction/speculative execution mechanism requiring extra overheads, drastically diminishing the presented speedup values.
In this experiment, the overheads of a prediction/speculative execution mechanism are not considered.
Hence, the presented numbers are best-case scenarios and are not realisable in a concrete implementation.

We have `r format(nrow(data), big.mark=",")` blocks, of which `r format(nrow(parallelisable), big.mark=",")` are parallelisable.
The number of strictly sequential blocks (not parallelisable) is `r format(100.0*(nrow(data)-nrow(parallelisable))/nrow(data), digits=4)`%.
The maximal achievable parallelism in a block is `r format(max(data$speedup), digits=4)`.
The geometric mean of the speedup is **`r format(exp(mean(log(data$speedup))), digits=4)`**.

We show the distribution of the speedup for blocks that are not sequential.
Sequential blocks whose speedup is one are omitted from the following distribution:

```{r, echo = FALSE, message=FALSE}
hist(parallelisable$speedup, main="Speedup Distribution without Sequential Blocks", breaks = quantile(parallelisable$speedup, 0:10 / 10), xlab="Speedup", col="lightblue1")
lines(density(parallelisable$speedup), col="dodgerblue3", lwd=2)
```

The following figure shows the speedup over block height.
For this distribution, we did not filter the sequential blocks, i.e., all blocks are included for the following timeline:

```{r, echo = FALSE, message=FALSE}
reducedData %>%
  ggplot(aes(x = block, y = speedup)) +
  geom_smooth(color = "tomato") +
  geom_point(alpha=0.3) +
  labs(x="Block Height", y="Speedup")
```

## 3. Number of Transactions per Block

We have, on average `r mean(data$numTx)` transactions per block.
The smallest number of transactions per block is `r min(data$numTx)`, and the largest number is `r max(data$numTx)`.
The distribution of number of transactions per block is shown below:

```{r, echo = FALSE, message=FALSE}
hist(data$numTx, freq=FALSE, main="Number of Transaction Distribution", xlab="Number of Transactions", col="lightblue1"); rug(data$numTx)
abline(v=mean(data$numTx), col="dodgerblue3", lty=2, lwd=2)
```

## 4. Processor Utilisation / Efficiency

We measure the processor utilisation of a block by constructing the task graph for a block.
The width of the task graph is a proxy for the maximum number of processors that may be required to achieve the speedup.
The ratio between speedup and width gives an upper bound on the processor utilisation (aka. efficiency).
However, a precise measure of efficiency is NP-hard problem and cannot be computed in polynomial time.

The lower bound of the processor utilisation is `r format(100.0*exp(mean(log(data$speedup/data$ubNumProc))), digits=4)`% on average, with a maximum of `r max(data$ubNumProc)` processors (upper bound).

## 5. Sequential Block Time

The average block execution time is `r mean(data$tBlock)/1e6` milliseconds with a minimum of `r min(data$tBlock)/1e3` microseconds and a maximum of `r max(data$tBlock)/1e9` seconds.
The smoothened trend line for block time (in milliseconds) is shown below:

```{r, echo=FALSE, message=FALSE}
reducedData %>%
  ggplot(aes(x = block, y = tBlock)) +
  geom_smooth(color = "tomato") +
  geom_point(alpha=0.3) +
  labs(x="Block Height", y="Block Time (ms)", title="Sequential Block Time")
```

## 6. Commit Time

The average commit time is `r mean(data$tCommit)/1e6` milliseconds with a minimum of `r min(data$tCommit)/1e3` microseconds and a maximum of `r max(data$tCommit)/1e9` seconds.
The smoothened trend line of the commit time in milliseconds is shown below:

```{r, echo = FALSE, message=FALSE}
reducedData %>%
  ggplot(aes(x = block, y = tCommit)) +
  geom_smooth(color = "tomato") +
  geom_point(alpha=0.3) +
  labs(x="Block Height", y="Commit Time (ms)", title="Commit Time")
```

## 7. Block gas

The average block gas is `r mean(data$blockGas)`
The smoothened trend line of the commit time in milliseconds is shown below:

```{r, echo = FALSE, message=FALSE}
reducedData %>%
  ggplot(aes(x = block, y = blockGas)) +
  geom_smooth(color = "tomato") +
  geom_point(alpha=0.3) +
  labs(x="Block Height", y="Block Gas", title=Block gas")
```
